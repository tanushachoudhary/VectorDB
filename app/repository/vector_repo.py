import chromadb
from typing import List, Dict, Optional, Any, Tuple
import json
from pathlib import Path
from app.models.schemas import ChunkModel, MetadataModel, MetadataFilter
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)


class VectorRepository:
    """Repository layer for vector database operations using Chroma."""
    
    def __init__(self):
        """Initialize Chroma client with persistence."""
        # Ensure persist directory exists
        persist_dir = Path(settings.chroma_persist_directory)
        persist_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize Chroma persistent client (new API)
        self.client = chromadb.PersistentClient(path=str(persist_dir))
        self.collection = self._get_or_create_collection()
        logger.info(f"Vector repository initialized with collection: {settings.collection_name}")
    
    def _get_or_create_collection(self):
        """Get or create the collection."""
        collection = self.client.get_or_create_collection(
            name=settings.collection_name,
            metadata={"hnsw:space": "cosine"}
        )
        return collection
    
    def index_chunks(self, chunks: List[ChunkModel]) -> Dict[str, Any]:
        """
        Index document chunks into the vector database.
        
        Args:
            chunks: List of ChunkModel objects to index
            
        Returns:
            Dictionary with indexing results
        """
        if not chunks:
            raise ValueError("Cannot index empty chunks list")
        
        try:
            # Prepare data for Chroma
            ids = []
            embeddings = []
            documents = []
            metadatas = []
            
            for chunk in chunks:
                ids.append(chunk.chunk_id)
                documents.append(chunk.content)
                
                # Serialize metadata to JSON
                metadata_dict = {
                    "document_id": chunk.document_id,
                    "user_id": chunk.user_id,
                    "source": chunk.metadata.source,
                    "page_number": chunk.metadata.page_number,
                    "chunk_index": chunk.metadata.chunk_index,
                    "created_at": chunk.metadata.created_at,
                    "tags": json.dumps(chunk.metadata.tags),  # JSON serialize tags
                }
                metadatas.append(metadata_dict)
            
            # Add to collection (embeddings will be generated by Chroma)
            self.collection.add(
                ids=ids,
                documents=documents,
                metadatas=metadatas,
            )
            
            logger.info(f"Successfully indexed {len(chunks)} chunks")
            
            return {
                "status": "success",
                "chunks_indexed": len(chunks),
                "first_chunk_id": ids[0] if ids else None
            }
        
        except Exception as e:
            logger.error(f"Error indexing chunks: {str(e)}")
            raise
    
    def semantic_search(self, query: str, top_k: int = 5) -> List[Tuple[ChunkModel, float]]:
        """
        Perform semantic search using vector similarity.
        
        Args:
            query: Search query text
            top_k: Number of results to return
            
        Returns:
            List of (ChunkModel, similarity_score) tuples
        """
        if not query or not query.strip():
            raise ValueError("Query cannot be empty")
        
        if top_k < 1:
            raise ValueError("top_k must be at least 1")
        
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=top_k,
                include=["embeddings", "documents", "metadatas", "distances"]
            )
            
            # Convert distances to similarity scores (cosine distance to similarity)
            # distance in [0, 2], similarity = 1 - (distance / 2)
            search_results = []
            
            if results and results["ids"] and results["ids"][0]:
                for i, chunk_id in enumerate(results["ids"][0]):
                    distance = results["distances"][0][i]
                    # Convert cosine distance to similarity score
                    similarity_score = 1 - (distance / 2)
                    
                    metadata_dict = results["metadatas"][0][i]
                    content = results["documents"][0][i]
                    
                    # Reconstruct ChunkModel
                    metadata = MetadataModel(
                            source=metadata_dict.get("source", "unknown"),
                            page_number=metadata_dict.get("page_number", 1),
                            chunk_index=metadata_dict.get("chunk_index", 0),
                            created_at=metadata_dict.get("created_at", ""),
                            tags=json.loads(metadata_dict.get("tags", "[]")) if metadata_dict.get("tags") else []
                    )
                    
                    chunk = ChunkModel(
                        chunk_id=chunk_id,
                            document_id=metadata_dict.get("document_id", ""),
                            user_id=metadata_dict.get("user_id", ""),
                        content=content,
                        metadata=metadata
                    )
                    
                    search_results.append((chunk, similarity_score))
            
            logger.info(f"Semantic search for '{query}' returned {len(search_results)} results")
            return search_results
        
        except Exception as e:
            logger.error(f"Error during semantic search: {str(e)}")
            raise
    
    def metadata_search(self, filters: MetadataFilter, top_k: int = 5) -> List[Tuple[ChunkModel, float]]:
        """
        Search by metadata filters without vector similarity.
        
        Args:
            filters: MetadataFilter object with filter criteria
            top_k: Number of results to return
            
        Returns:
            List of (ChunkModel, score) tuples (score = 1.0 for metadata matches)
        """
        try:
            # Build where clause for Chroma
            where_conditions = []
            
            if filters.source:
                where_conditions.append({"source": {"$eq": filters.source}})
            
            if filters.page_number is not None:
                where_conditions.append({"page_number": {"$eq": filters.page_number}})
            
            if filters.document_id:
                where_conditions.append({"document_id": {"$eq": filters.document_id}})
            
            if filters.user_id:
                where_conditions.append({"user_id": {"$eq": filters.user_id}})
            
            # Combine conditions with AND
            where_clause = None
            if where_conditions:
                where_clause = {"$and": where_conditions} if len(where_conditions) > 1 else where_conditions[0]
            
            # Query with filters
            results = self.collection.get(
                where=where_clause,
                limit=top_k,
                include=["documents", "metadatas"]
            )
            
            # Reconstruct ChunkModel objects
            search_results = []
            
            if results and results["ids"]:
                for i, chunk_id in enumerate(results["ids"]):
                    metadata_dict = results["metadatas"][i]
                    content = results["documents"][i]
                    
                    metadata = MetadataModel(
                            source=metadata_dict.get("source", "unknown"),
                            page_number=metadata_dict.get("page_number", 1),
                            chunk_index=metadata_dict.get("chunk_index", 0),
                            created_at=metadata_dict.get("created_at", ""),
                            tags=json.loads(metadata_dict.get("tags", "[]")) if metadata_dict.get("tags") else []
                    )
                    
                    chunk = ChunkModel(
                        chunk_id=chunk_id,
                            document_id=metadata_dict.get("document_id", ""),
                            user_id=metadata_dict.get("user_id", ""),
                        content=content,
                        metadata=metadata
                    )
                    
                    search_results.append((chunk, 1.0))
            
            logger.info(f"Metadata search returned {len(search_results)} results")
            return search_results
        
        except Exception as e:
            logger.error(f"Error during metadata search: {str(e)}")
            raise
    
    def hybrid_search(
        self, 
        query: str, 
        filters: MetadataFilter, 
        top_k: int = 5, 
        weight_vector: float = 0.7
    ) -> List[Tuple[ChunkModel, float]]:
        """
        Perform hybrid search combining vector similarity and metadata filters.
        
        Args:
            query: Search query text
            filters: MetadataFilter for metadata constraints
            top_k: Number of results to return
            weight_vector: Weight for vector search score (0-1)
            
        Returns:
            List of (ChunkModel, combined_score) tuples
        """
        if not query or not query.strip():
            raise ValueError("Query cannot be empty")
        
        if not (0 <= weight_vector <= 1):
            raise ValueError("weight_vector must be between 0 and 1")
        
        try:
            # Get semantic search results
            vector_results = self.semantic_search(query, top_k=top_k * 2)
            
            # Get metadata filter results
            metadata_results = self.metadata_search(filters, top_k=top_k * 2)
            metadata_ids = {chunk.chunk_id for chunk, _ in metadata_results}
            
            # Combine results
            combined_results = {}
    
            # vector_results is a list of tuples: [(chunk_obj, score), ...]
            for chunk, similarity_score in vector_results:
                if chunk.chunk_id in metadata_ids:
                    # Calculate weighted score
                    combined_score = (similarity_score * weight_vector) + (1.0 * (1 - weight_vector))
                    combined_results[chunk.chunk_id] = (chunk, combined_score)
            
            # Sort by score and limit to top_k
            sorted_results = sorted(
                combined_results.items(),
                key=lambda x: x[1][1],
                reverse=True
            )[:top_k]
            
            results = [(chunk, score) for _, (chunk, score) in sorted_results]
            logger.info(f"Hybrid search returned {len(results)} results")
            return results
        
        except Exception as e:
            logger.error(f"Error during hybrid search: {str(e)}")
            raise
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get statistics about the vector database.
        
        Returns:
            Dictionary with database statistics
        """
        try:
            count = self.collection.count()
            
            # Get all unique document and user IDs
            all_data = self.collection.get(include=["metadatas"])
            
            document_ids = set()
            user_ids = set()
            
            if all_data and all_data["metadatas"]:
                for metadata in all_data["metadatas"]:
                    document_ids.add(metadata.get("document_id"))
                    user_ids.add(metadata.get("user_id"))
            
            stats = {
                "total_chunks": count,
                "total_documents": len(document_ids),
                "total_users": len(user_ids),
                "collection_name": settings.collection_name,
                "embedding_dimension": settings.embedding_dimension,
                "chunk_size": settings.chunk_size,
                "chunk_overlap": settings.chunk_overlap,
                "embedding_model": settings.embedding_model
            }
            
            logger.info(f"Vector DB stats: {count} chunks from {len(document_ids)} documents")
            return stats
        
        except Exception as e:
            logger.error(f"Error retrieving stats: {str(e)}")
            raise
    
    def delete_chunk(self, chunk_id: str) -> bool:
        """Delete a chunk by ID."""
        try:
            self.collection.delete(ids=[chunk_id])
            logger.info(f"Deleted chunk: {chunk_id}")
            return True
        except Exception as e:
            logger.error(f"Error deleting chunk: {str(e)}")
            raise
    
    def delete_document(self, document_id: str) -> int:
        """Delete all chunks for a document."""
        try:
            results = self.collection.get(
                where={"document_id": {"$eq": document_id}},
                include=[]
            )
            
            if results["ids"]:
              if results and results.get("ids"):
                 self.collection.delete(ids=results["ids"])
                 logger.info(f"Deleted {len(results['ids'])} chunks for document: {document_id}")
                 return len(results["ids"])
            
            return 0
        
        except Exception as e:
            logger.error(f"Error deleting document: {str(e)}")
            raise
